---
title: "Assignment 04: Mixture Models and Data Visualization"
author: "Aaron Riedling"
---

# Introduction
In this assignment, you will explore data visualization using the ggplot2
library and implement a Gaussian Mixture Model using the
Expectation-Maximization (EM) algorithm.

## Task 0: Data Generation
This section provides the data for your analysis.
The code below generates synthetic data that simulates a mixture of normal
distributions, similar to what you might encounter in real biomedical data
(e.g., gene expression levels from different cell populations).

```{r}
generate_mixture_data <- function(n1 = 200, n2 = 300, mu1 = 2, mu2 = 5, 
                                sigma1 = 0.5, sigma2 = 1) {
  set.seed(123)  # For reproducibility
  data1 <- rnorm(n1, mean = mu1, sd = sigma1)
  data2 <- rnorm(n2, mean = mu2, sd = sigma2)
  return(c(data1, data2))
}

# Generate data for use in subsequent tasks
mixture_data <- generate_mixture_data()
```

## Task 1: Data Visualization with ggplot2 (30 pts)
The ggplot2 library implements the "Grammar of Graphics," a systematic
approach to creating visualizations by combining fundamental components.
This approach allows for highly customizable and professional-looking plots.

**Resources:**
- [ggplot2 Documentation](https://ggplot2.tidyverse.org/reference/)
- [R Graphics Cookbook](https://r-graphics.org/)

Complete the following visualization tasks:

1.1 Create histograms

- Plot histograms for both components of the mixture distribution
- Use different colors for each component
- Add proper axis labels and title
- Implement transparency using the alpha parameter
- Add a legend

1.2 Add density curves

- Overlay density curves on the histograms
- Include the overall mixture density
- Ensure proper color matching with histograms
- Add appropriate legend entries
- You may reuse the parameters from Task 0

1.3 Mode identification

The mode is the value that appears most frequently in the distribution.

- Calculate and plot the modes of each component (add a vertical line or point)
- Add text labels showing the x and y coordinates

```{r}
# Load required libraries
library(ggplot2)
suppressWarnings(suppressMessages(library(tidyverse)))



```

```{r task1_plotting}
# Your solution here!

# Generate mixture data
set.seed(123)
data1 <- rnorm(200, mean = 2, sd = 0.5)
data2 <- rnorm(300, mean = 5, sd = 1)

# Combine into a single data frame
mixture_df <- data.frame(
  value = c(data1, data2),
  component = rep(c("Component 1", "Component 2"), times = c(length(data1), length(data2)))
)

# Create histograms
ggplot(mixture_df, aes(x = value, fill = component)) +
  geom_histogram(position = "identity", bins = 30, alpha = 0.6) +
  scale_fill_manual(values = c("blue", "red")) +
  labs(
    title = "Histograms of Mixture Components",
    x = "Value",
    y = "Frequency",
    fill = "Component"
  ) +
  theme_minimal()


# Plot histograms with density curves
ggplot(mixture_df, aes(x = value, fill = component)) +
  geom_histogram(aes(y = ..density..), position = "identity", bins = 30, alpha = 0.6) +
  geom_density(aes(color = component), size = 1, alpha = 0.8) +
  stat_density(geom = "line", color = "black", size = 1, alpha = 0.8) + # Overall density
  scale_fill_manual(values = c("blue", "red")) +
  scale_color_manual(values = c("blue", "red")) +
  labs(
    title = "Histograms with Density Curves",
    x = "Value",
    y = "Density",
    fill = "Component",
    color = "Component"
  ) +
  theme_minimal()


# Calculate modes
mode1 <- density(data1)$x[which.max(density(data1)$y)]
mode2 <- density(data2)$x[which.max(density(data2)$y)]

# Annotate plot with modes
ggplot(mixture_df, aes(x = value, fill = component)) +
  geom_histogram(aes(y = ..density..), position = "identity", bins = 30, alpha = 0.6) +
  geom_density(aes(color = component), size = 1, alpha = 0.8) +
  stat_density(geom = "line", color = "black", size = 1, alpha = 0.8) + # Overall density
  geom_vline(xintercept = mode1, color = "blue", linetype = "dashed", size = 1) +
  geom_vline(xintercept = mode2, color = "red", linetype = "dashed", size = 1) +
  annotate("text", x = mode1, y = 0.2, label = paste("Mode1:", round(mode1, 2)), color = "blue") +
  annotate("text", x = mode2, y = 0.2, label = paste("Mode2:", round(mode2, 2)), color = "red") +
  labs(
    title = "Density Curves with Modes Annotated",
    x = "Value",
    y = "Density",
    fill = "Component",
    color = "Component"
  ) +
  theme_minimal()














```

## Task 2: Implementing the EM Algorithm (70 pts)
The Expectation-Maximization algorithm is a powerful method for fitting mixture
models to data. Your task is to implement the EM algorithm for Gaussian mixture
models by following the steps outlined below.

2.1 Initialize parameters

- Initialize means (μ), standard deviations (σ), and mixing proportions (λ)
- Initialize a matrix for the responsibilities (i.e. the probability that a data point comes from a distribution, one row per data point, one column per distribution)
- Initialize log-likelihood storage

2.2 Implement EM algorithm

Loop until convergence:

- E-step: Calculate responsibilities
- M-step: Update parameters
- Calculate log-likelihood
- Check convergence criterion

2.3 Return results

Return a list containing:

- Final parameter estimates
- Responsibility matrix
- Log-likelihood history
- Number of iterations

2.4 Plot density estimate

Use the true parameters (task 0) and the estimated paramters to plot and compare the 
density estimate of the Gaussian mixture model.


Create a function `em_gaussian_mixture` that implements the EM algorithm for a
Gaussian mixture model. The function should take the following arguments:

- `data`: A vector of data points
- `k`: The number of components in the mixture model(optional, default is 2)
- `max_iter`: The maximum number of iterations
- `tolerance`: The convergence criterion based on the change in log-likelihood

it should return a list with the following components:

- μ: The estimated means of the components
- σ: The estimated standard deviations of the components
- λ: The estimated mixing proportions

```{r task2_expectation_maximization}
# Your solution here!
em_gaussian_mixture <- function(data, k = 2, max_iter = 100, tolerance = 1e-6) {
  # Number of data points
  n <- length(data)
  
  # 2.1 Initialize parameters
  set.seed(123) # For reproducibility
  mu <- sample(data, k) # Randomly select initial means
  sigma <- rep(sd(data), k) # Initialize standard deviations to overall std. dev
  lambda <- rep(1/k, k) # Initialize mixing proportions equally
  responsibilities <- matrix(0, nrow = n, ncol = k) # Responsibilities matrix
  log_likelihoods <- numeric(max_iter) # Log-likelihood storage
  
  # 2.2 Implement EM algorithm
  for (iter in 1:max_iter) {
    # E-step: Calculate responsibilities
    for (j in 1:k) {
      responsibilities[, j] <- lambda[j] * dnorm(data, mean = mu[j], sd = sigma[j])
    }
    responsibilities <- responsibilities / rowSums(responsibilities)
    
    # M-step: Update parameters
    for (j in 1:k) {
      rj_sum <- sum(responsibilities[, j]) # Sum of responsibilities for component j
      mu[j] <- sum(responsibilities[, j] * data) / rj_sum
      sigma[j] <- sqrt(sum(responsibilities[, j] * (data - mu[j])^2) / rj_sum)
      lambda[j] <- rj_sum / n
    }
    
    # Calculate log-likelihood
    log_likelihood <- sum(log(rowSums(sapply(1:k, function(j) {
      lambda[j] * dnorm(data, mean = mu[j], sd = sigma[j])
    }))))
    log_likelihoods[iter] <- log_likelihood
    
    # Check convergence
    if (iter > 1 && abs(log_likelihoods[iter] - log_likelihoods[iter - 1]) < tolerance) {
      log_likelihoods <- log_likelihoods[1:iter]
      break
    }
  }
  
  # Assign labels for clarity
  names(mu) <- paste0("Component ", 1:k)
  names(sigma) <- paste0("Component ", 1:k)
  names(lambda) <- paste0("Component ", 1:k)
  
  # 2.3 Return results
  return(list(
    mu = mu,
    sigma = sigma,
    lambda = lambda,
    responsibilities = responsibilities,
    log_likelihoods = log_likelihoods,
    iterations = iter
  ))
}

# 2.4 Plot density estimate
plot_density <- function(data, true_mu, true_sigma, true_lambda, estimated_mu, estimated_sigma, estimated_lambda) {
  hist(data, breaks = 30, probability = TRUE, main = "Density Estimation", col = "gray", border = "white")
  x_vals <- seq(min(data), max(data), length.out = 1000)
  
  # True density
  true_density <- rowSums(sapply(1:length(true_mu), function(j) {
    true_lambda[j] * dnorm(x_vals, mean = true_mu[j], sd = true_sigma[j])
  }))
  lines(x_vals, true_density, col = "blue", lwd = 2, lty = 2)
  
  # Estimated density
  estimated_density <- rowSums(sapply(1:length(estimated_mu), function(j) {
    estimated_lambda[j] * dnorm(x_vals, mean = estimated_mu[j], sd = estimated_sigma[j])
  }))
  lines(x_vals, estimated_density, col = "red", lwd = 2)
  
  legend("topright", legend = c("True Density", "Estimated Density"), 
         col = c("blue", "red"), lwd = 2, lty = c(2, 1))
}

# Example Usage
set.seed(42)
data <- c(rnorm(100, mean = -2, sd = 1), rnorm(100, mean = 3, sd = 1.5))
results <- em_gaussian_mixture(data, k = 2)

# Print results
cat("Estimated Means (μ):\n")
print(results$mu)

cat("\nEstimated Standard Deviations (σ):\n")
print(results$sigma)

cat("\nEstimated Mixing Proportions (λ):\n")
print(results$lambda)

# True parameters for comparison
true_mu <- c(-2, 3)
true_sigma <- c(1, 1.5)
true_lambda <- c(0.5, 0.5)

# Plot the density
plot_density(data, true_mu, true_sigma, true_lambda, results$mu, results$sigma, results$lambda)











```
